cbow:
  left_window_size: 8
  right_window_size: 0
ed:
  embedding_dim: 128
  hidden_dim: 256
  num_layers: 1
  fc_dim: 1024
tf:
  embedding_dim: 200
  num_head: 2
  hidden_dim: 200
  num_layers: 2
  dropout: 0.1
pe:
  dropout: 0.1
  max_len: 1000
training:
  lr: 0.001
  batch_size: 32
  momentum: 0
  weight_decay: 0
  num_epochs: 2
  saves_folder: "saves"
  validate_on_epoch: 1
data:
  bruk_path: "data/raw/bruk"
  uc_path: "data/raw/ubercorpus/news.lemmatized.shuffled.txt"
  stopwords: "data/stopwords/stopwords_ua.txt"
  tokenizer_path: "data/processed/uc_005_w8_v10000/tokenizer.json"
  context_path: "data/processed/uc_005_w8_v10000/context.pt"
  target_path: "data/processed/uc_005_w8_v10000/target.pt"
  train_size: 1200
  test_size: 105
  vocab_size: 40000
  unk_token: "~"
  uc_ratio: 0.05