tf:
  embedding_dim: 128
  num_head: 8
  num_encoder_layers: 8
  num_decoder_layers: 4
  dropout_prob: 0.1
pe:
  dropout_prob: 0.1
  max_len: 1000
training:
  lr: 1
  batch_size: 128
  num_epochs: 100
  saves_folder: "/content/gdrive/MyDrive/UnikEbaniy/ukr_word_embeddings/data_and_saves"
  validate_on_epoch: 1
  train_ratio: 0.99
data:
  uc_path: "data/raw/ubercorpus/news.lemmatized.shuffled.txt"
  stopwords: "data/stopwords/stopwords_ua.txt"
  tokenizer_path: "data/processed/uc_full_w8_v10000/tokenizer.json"
  context_path: "data/processed/uc_full_w8_v10000/context.pt"
  target_path: "data/processed/uc_full_w8_v10000/target.pt"
  vocab_size: 10000
  unk_token: "~"
  uc_ratio: 1
  window_size: 8